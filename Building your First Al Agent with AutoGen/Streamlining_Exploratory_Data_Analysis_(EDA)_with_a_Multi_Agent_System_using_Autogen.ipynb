{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyautogen google-generativeai pandas matplotlib autogen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_nuNW2lBU2V",
        "outputId": "4f263ac7-b05c-49ae-d142-0b00504a28fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyautogen in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting autogen\n",
            "  Downloading autogen-0.9.6-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: autogen-agentchat>=0.6.4 in /usr/local/lib/python3.11/dist-packages (from pyautogen) (0.6.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.176.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Collecting ag2==0.9.6 (from autogen)\n",
            "  Downloading ag2-0.9.6-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (4.9.0)\n",
            "Collecting asyncer==0.0.8 (from ag2==0.9.6->autogen)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting diskcache (from ag2==0.9.6->autogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from ag2==0.9.6->autogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (0.28.1)\n",
            "Collecting python-dotenv (from ag2==0.9.6->autogen)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (3.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from ag2==0.9.6->autogen) (0.9.0)\n",
            "Requirement already satisfied: autogen-core==0.6.4 in /usr/local/lib/python3.11/dist-packages (from autogen-agentchat>=0.6.4->pyautogen) (0.6.4)\n",
            "Requirement already satisfied: jsonref~=1.1.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.4->autogen-agentchat>=0.6.4->pyautogen) (1.1.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.6.4->autogen-agentchat>=0.6.4->pyautogen) (1.35.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.6->autogen) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->ag2==0.9.6->autogen) (1.3.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->ag2==0.9.6->autogen) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->ag2==0.9.6->autogen) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.28.1->ag2==0.9.6->autogen) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->ag2==0.9.6->autogen) (2024.11.6)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.34.1->autogen-core==0.6.4->autogen-agentchat>=0.6.4->pyautogen) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.34.1->autogen-core==0.6.4->autogen-agentchat>=0.6.4->pyautogen) (3.23.0)\n",
            "Downloading autogen-0.9.6-py3-none-any.whl (13 kB)\n",
            "Downloading ag2-0.9.6-py3-none-any.whl (859 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.2/859.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, diskcache, docker, asyncer, ag2, autogen\n",
            "Successfully installed ag2-0.9.6 asyncer-0.0.8 autogen-0.9.6 diskcache-5.6.3 docker-7.1.0 python-dotenv-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Mxylx1AA8Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Any, Optional\n",
        "import autogen_agentchat as autogen\n",
        "from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager\n",
        "import google.generativeai as genai\n",
        "from io import StringIO\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration for Gemini\n",
        "class GeminiConfig:\n",
        "    def __init__(self, api_key: str, model_name: str = \"gemini-1.5-flash\"):\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def get_config(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"model\": self.model_name,\n",
        "            \"api_key\": \"your_key\",\n",
        "            \"api_type\": \"gemini\"\n",
        "        }\n",
        "\n",
        "class EDAMultiAgentSystem:\n",
        "    def __init__(self, gemini_api_key: str):\n",
        "        self.gemini_config = GeminiConfig(gemini_api_key)\n",
        "        self.agents = {}\n",
        "        self.data = None\n",
        "        self.results = {}\n",
        "        self.setup_agents()\n",
        "\n",
        "    def setup_agents(self):\n",
        "        \"\"\"Initialize all agents with their specific roles and configurations\"\"\"\n",
        "\n",
        "        # Base configuration for all agents\n",
        "        base_config = {\n",
        "            \"config_list\": [self.gemini_config.get_config()],\n",
        "            \"temperature\": 0.1,\n",
        "            \"timeout\": 300,\n",
        "        }\n",
        "\n",
        "        # 1. Data Preparation Agent\n",
        "        self.agents['data_prep'] = AssistantAgent(\n",
        "            name=\"DataPrepAgent\",\n",
        "            system_message=\"\"\"You are a Data Preparation Agent specialized in data cleaning and preprocessing.\n",
        "            Your responsibilities include:\n",
        "            - Loading and inspecting datasets\n",
        "            - Handling missing values\n",
        "            - Detecting and treating outliers\n",
        "            - Data type conversions\n",
        "            - Feature engineering basics\n",
        "            - Data validation and quality checks\n",
        "\n",
        "            Always provide clear explanations for your preprocessing decisions and document any assumptions made.\n",
        "            Return clean, well-structured data ready for analysis.\"\"\",\n",
        "            llm_config=base_config\n",
        "        )\n",
        "\n",
        "        # 2. EDA Agent\n",
        "        self.agents['eda'] = AssistantAgent(\n",
        "            name=\"EDAAgent\",\n",
        "            system_message=\"\"\"You are an Exploratory Data Analysis Agent specialized in statistical analysis and visualization.\n",
        "            Your responsibilities include:\n",
        "            - Generating descriptive statistics\n",
        "            - Creating informative visualizations\n",
        "            - Identifying patterns and trends\n",
        "            - Correlation analysis\n",
        "            - Distribution analysis\n",
        "            - Feature importance assessment\n",
        "\n",
        "            Focus on generating actionable insights and clear visualizations that tell a story about the data.\n",
        "            Always explain what the visualizations reveal about the data.\"\"\",\n",
        "            llm_config=base_config\n",
        "        )\n",
        "\n",
        "        # 3. Report Generator Agent\n",
        "        self.agents['report'] = AssistantAgent(\n",
        "            name=\"ReportAgent\",\n",
        "            system_message=\"\"\"You are a Report Generator Agent specialized in creating comprehensive EDA reports.\n",
        "            Your responsibilities include:\n",
        "            - Structuring findings into a coherent report\n",
        "            - Summarizing key insights and discoveries\n",
        "            - Creating executive summaries\n",
        "            - Organizing visualizations with clear captions\n",
        "            - Providing actionable recommendations\n",
        "\n",
        "            Create well-formatted, professional reports that are accessible to both technical and non-technical audiences.\n",
        "            Include methodology, findings, and recommendations sections.\"\"\",\n",
        "            llm_config=base_config\n",
        "        )\n",
        "\n",
        "        # 4. Critic Agent\n",
        "        self.agents['critic'] = AssistantAgent(\n",
        "            name=\"CriticAgent\",\n",
        "            system_message=\"\"\"You are a Critic Agent responsible for quality assurance and feedback.\n",
        "            Your responsibilities include:\n",
        "            - Reviewing analysis quality and accuracy\n",
        "            - Checking for logical inconsistencies\n",
        "            - Ensuring completeness of analysis\n",
        "            - Providing constructive feedback\n",
        "            - Suggesting improvements\n",
        "            - Validating conclusions\n",
        "\n",
        "            Be thorough but constructive in your feedback. Focus on improving clarity, accuracy, and actionability.\n",
        "            Always provide specific suggestions for improvement.\"\"\",\n",
        "            llm_config=base_config\n",
        "        )\n",
        "\n",
        "        # 5. Executor Agent\n",
        "        self.agents['executor'] = AssistantAgent(\n",
        "            name=\"ExecutorAgent\",\n",
        "            system_message=\"\"\"You are an Executor Agent responsible for code validation and result verification.\n",
        "            Your responsibilities include:\n",
        "            - Executing and validating code\n",
        "            - Checking for errors and bugs\n",
        "            - Ensuring reproducibility\n",
        "            - Verifying statistical calculations\n",
        "            - Testing code functionality\n",
        "\n",
        "            Focus on technical accuracy and ensure all code runs correctly and produces valid results.\n",
        "            Report any issues or inconsistencies found during execution.\"\"\",\n",
        "            llm_config=base_config\n",
        "        )\n",
        "\n",
        "        # 6. Admin Agent (User Proxy)\n",
        "        self.agents['admin'] = UserProxyAgent(\n",
        "            name=\"AdminAgent\",\n",
        "            system_message=\"\"\"You are the Admin Agent overseeing the EDA workflow.\n",
        "            Your responsibilities include:\n",
        "            - Coordinating between agents\n",
        "            - Managing the overall workflow\n",
        "            - Ensuring alignment with project goals\n",
        "            - Making final decisions on analysis direction\n",
        "            - Facilitating communication between agents\n",
        "\n",
        "            Keep the analysis focused and ensure all agents work toward the common goal of comprehensive EDA.\"\"\",\n",
        "            human_input_mode=\"NEVER\",\n",
        "            max_consecutive_auto_reply=3,\n",
        "            code_execution_config={\"work_dir\": \"eda_output\", \"use_docker\": False}\n",
        "        )\n",
        "\n",
        "    def load_data(self, data_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Load data from various formats\"\"\"\n",
        "        try:\n",
        "            if data_path.endswith('.csv'):\n",
        "                self.data = pd.read_csv(data_path)\n",
        "            elif data_path.endswith('.xlsx') or data_path.endswith('.xls'):\n",
        "                self.data = pd.read_excel(data_path)\n",
        "            elif data_path.endswith('.json'):\n",
        "                self.data = pd.read_json(data_path)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "            print(f\"Data loaded successfully. Shape: {self.data.shape}\")\n",
        "            return self.data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_sample_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Create sample data for demonstration\"\"\"\n",
        "        np.random.seed(42)\n",
        "        n_samples = 1000\n",
        "\n",
        "        self.data = pd.DataFrame({\n",
        "            'age': np.random.randint(18, 80, n_samples),\n",
        "            'income': np.random.normal(50000, 15000, n_samples),\n",
        "            'education_years': np.random.randint(8, 20, n_samples),\n",
        "            'experience_years': np.random.randint(0, 40, n_samples),\n",
        "            'satisfaction_score': np.random.uniform(1, 10, n_samples),\n",
        "            'department': np.random.choice(['Engineering', 'Sales', 'Marketing', 'HR'], n_samples),\n",
        "            'city': np.random.choice(['New York', 'San Francisco', 'Chicago', 'Boston'], n_samples),\n",
        "            'performance_rating': np.random.choice(['Poor', 'Fair', 'Good', 'Excellent'], n_samples)\n",
        "        })\n",
        "\n",
        "        # Add some missing values\n",
        "        missing_indices = np.random.choice(n_samples, size=int(0.05 * n_samples), replace=False)\n",
        "        self.data.loc[missing_indices, 'satisfaction_score'] = np.nan\n",
        "\n",
        "        # Add some outliers\n",
        "        outlier_indices = np.random.choice(n_samples, size=20, replace=False)\n",
        "        self.data.loc[outlier_indices, 'income'] = np.random.uniform(200000, 500000, 20)\n",
        "\n",
        "        print(f\"Sample data created successfully. Shape: {self.data.shape}\")\n",
        "        return self.data\n",
        "\n",
        "    def setup_group_chat(self) -> GroupChatManager:\n",
        "        \"\"\"Setup group chat for agent collaboration\"\"\"\n",
        "        agents_list = list(self.agents.values())\n",
        "\n",
        "        group_chat = GroupChat(\n",
        "            agents=agents_list,\n",
        "            messages=[],\n",
        "            max_round=20,\n",
        "            speaker_selection_method=\"round_robin\"\n",
        "        )\n",
        "\n",
        "        manager = GroupChatManager(\n",
        "            groupchat=group_chat,\n",
        "            llm_config={\"config_list\": [self.gemini_config.get_config()]},\n",
        "            name=\"EDAManager\"\n",
        "        )\n",
        "\n",
        "        return manager\n",
        "\n",
        "    def run_data_preparation(self) -> Dict[str, Any]:\n",
        "        \"\"\"Run data preparation phase\"\"\"\n",
        "        print(\"Starting Data Preparation Phase...\")\n",
        "\n",
        "        if self.data is None:\n",
        "            print(\"No data loaded. Creating sample data...\")\n",
        "            self.create_sample_data()\n",
        "\n",
        "        # Data preparation tasks\n",
        "        prep_tasks = {\n",
        "            \"data_info\": self.get_data_info(),\n",
        "            \"missing_values\": self.handle_missing_values(),\n",
        "            \"outliers\": self.detect_outliers(),\n",
        "            \"data_types\": self.optimize_data_types()\n",
        "        }\n",
        "\n",
        "        self.results['data_preparation'] = prep_tasks\n",
        "        print(\"Data Preparation Phase completed.\")\n",
        "        return prep_tasks\n",
        "\n",
        "    def get_data_info(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get basic information about the dataset\"\"\"\n",
        "        info = {\n",
        "            \"shape\": self.data.shape,\n",
        "            \"columns\": list(self.data.columns),\n",
        "            \"dtypes\": self.data.dtypes.to_dict(),\n",
        "            \"memory_usage\": self.data.memory_usage(deep=True).sum(),\n",
        "            \"null_counts\": self.data.isnull().sum().to_dict()\n",
        "        }\n",
        "        return info\n",
        "\n",
        "    def handle_missing_values(self) -> Dict[str, Any]:\n",
        "        \"\"\"Handle missing values in the dataset\"\"\"\n",
        "        missing_summary = self.data.isnull().sum()\n",
        "        missing_percentage = (missing_summary / len(self.data)) * 100\n",
        "\n",
        "        # Simple imputation strategy\n",
        "        for column in self.data.columns:\n",
        "            if self.data[column].isnull().sum() > 0:\n",
        "                if self.data[column].dtype in ['int64', 'float64']:\n",
        "                    self.data[column].fillna(self.data[column].median(), inplace=True)\n",
        "                else:\n",
        "                    self.data[column].fillna(self.data[column].mode()[0], inplace=True)\n",
        "\n",
        "        return {\n",
        "            \"missing_counts\": missing_summary.to_dict(),\n",
        "            \"missing_percentage\": missing_percentage.to_dict(),\n",
        "            \"imputation_strategy\": \"median for numeric, mode for categorical\"\n",
        "        }\n",
        "\n",
        "    def detect_outliers(self) -> Dict[str, Any]:\n",
        "        \"\"\"Detect outliers using IQR method\"\"\"\n",
        "        numeric_columns = self.data.select_dtypes(include=[np.number]).columns\n",
        "        outlier_info = {}\n",
        "\n",
        "        for column in numeric_columns:\n",
        "            Q1 = self.data[column].quantile(0.25)\n",
        "            Q3 = self.data[column].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            outliers = ((self.data[column] < lower_bound) | (self.data[column] > upper_bound)).sum()\n",
        "            outlier_info[column] = {\n",
        "                \"count\": outliers,\n",
        "                \"percentage\": (outliers / len(self.data)) * 100,\n",
        "                \"bounds\": {\"lower\": lower_bound, \"upper\": upper_bound}\n",
        "            }\n",
        "\n",
        "        return outlier_info\n",
        "\n",
        "    def optimize_data_types(self) -> Dict[str, Any]:\n",
        "        \"\"\"Optimize data types for better performance\"\"\"\n",
        "        original_memory = self.data.memory_usage(deep=True).sum()\n",
        "\n",
        "        # Convert object columns to category if cardinality is low\n",
        "        for column in self.data.select_dtypes(include=['object']).columns:\n",
        "            if self.data[column].nunique() < 0.5 * len(self.data):\n",
        "                self.data[column] = self.data[column].astype('category')\n",
        "\n",
        "        new_memory = self.data.memory_usage(deep=True).sum()\n",
        "\n",
        "        return {\n",
        "            \"original_memory\": original_memory,\n",
        "            \"optimized_memory\": new_memory,\n",
        "            \"memory_reduction\": ((original_memory - new_memory) / original_memory) * 100,\n",
        "            \"optimized_dtypes\": self.data.dtypes.to_dict()\n",
        "        }\n",
        "\n",
        "    def run_eda_analysis(self) -> Dict[str, Any]:\n",
        "        \"\"\"Run comprehensive EDA analysis\"\"\"\n",
        "        print(\"Starting EDA Analysis Phase...\")\n",
        "\n",
        "        eda_results = {\n",
        "            \"descriptive_stats\": self.get_descriptive_statistics(),\n",
        "            \"correlations\": self.analyze_correlations(),\n",
        "            \"distributions\": self.analyze_distributions(),\n",
        "            \"categorical_analysis\": self.analyze_categorical_variables(),\n",
        "            \"insights\": self.generate_insights()\n",
        "        }\n",
        "\n",
        "        self.results['eda_analysis'] = eda_results\n",
        "        print(\"EDA Analysis Phase completed.\")\n",
        "        return eda_results\n",
        "\n",
        "    def get_descriptive_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"Generate descriptive statistics\"\"\"\n",
        "        numeric_stats = self.data.describe()\n",
        "        categorical_stats = self.data.describe(include=['object', 'category'])\n",
        "\n",
        "        return {\n",
        "            \"numeric_summary\": numeric_stats.to_dict(),\n",
        "            \"categorical_summary\": categorical_stats.to_dict() if not categorical_stats.empty else {},\n",
        "            \"unique_counts\": self.data.nunique().to_dict(),\n",
        "            \"value_counts\": {col: self.data[col].value_counts().head().to_dict()\n",
        "                             for col in self.data.select_dtypes(include=['object', 'category']).columns}\n",
        "        }\n",
        "\n",
        "    def analyze_correlations(self) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze correlations between numeric variables\"\"\"\n",
        "        numeric_data = self.data.select_dtypes(include=[np.number])\n",
        "        correlation_matrix = numeric_data.corr()\n",
        "\n",
        "        # Find strong correlations\n",
        "        strong_correlations = []\n",
        "        for i in range(len(correlation_matrix.columns)):\n",
        "            for j in range(i+1, len(correlation_matrix.columns)):\n",
        "                corr_value = correlation_matrix.iloc[i, j]\n",
        "                if abs(corr_value) > 0.5:  # Threshold for strong correlation\n",
        "                    strong_correlations.append({\n",
        "                        'var1': correlation_matrix.columns[i],\n",
        "                        'var2': correlation_matrix.columns[j],\n",
        "                        'correlation': corr_value\n",
        "                    })\n",
        "\n",
        "        return {\n",
        "            \"correlation_matrix\": correlation_matrix.to_dict(),\n",
        "            \"strong_correlations\": strong_correlations\n",
        "        }\n",
        "\n",
        "    def analyze_distributions(self) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze distributions of variables\"\"\"\n",
        "        distribution_analysis = {}\n",
        "\n",
        "        for column in self.data.select_dtypes(include=[np.number]).columns:\n",
        "            distribution_analysis[column] = {\n",
        "                \"mean\": self.data[column].mean(),\n",
        "                \"median\": self.data[column].median(),\n",
        "                \"std\": self.data[column].std(),\n",
        "                \"skewness\": self.data[column].skew(),\n",
        "                \"kurtosis\": self.data[column].kurtosis(),\n",
        "                \"min\": self.data[column].min(),\n",
        "                \"max\": self.data[column].max()\n",
        "            }\n",
        "\n",
        "        return distribution_analysis\n",
        "\n",
        "    def analyze_categorical_variables(self) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze categorical variables\"\"\"\n",
        "        categorical_analysis = {}\n",
        "\n",
        "        for column in self.data.select_dtypes(include=['object', 'category']).columns:\n",
        "            categorical_analysis[column] = {\n",
        "                \"unique_count\": self.data[column].nunique(),\n",
        "                \"most_frequent\": self.data[column].mode().iloc[0] if not self.data[column].mode().empty else None,\n",
        "                \"value_counts\": self.data[column].value_counts().to_dict(),\n",
        "                \"missing_count\": self.data[column].isnull().sum()\n",
        "            }\n",
        "\n",
        "        return categorical_analysis\n",
        "\n",
        "    def generate_insights(self) -> List[str]:\n",
        "        \"\"\"Generate key insights from the analysis\"\"\"\n",
        "        insights = []\n",
        "\n",
        "        # Data quality insights\n",
        "        missing_data = self.data.isnull().sum().sum()\n",
        "        if missing_data > 0:\n",
        "            insights.append(f\"Dataset contains {missing_data} missing values across all columns\")\n",
        "\n",
        "        # Distribution insights\n",
        "        numeric_columns = self.data.select_dtypes(include=[np.number]).columns\n",
        "        for column in numeric_columns:\n",
        "            skew = self.data[column].skew()\n",
        "            if abs(skew) > 1:\n",
        "                insights.append(f\"{column} shows {'positive' if skew > 0 else 'negative'} skewness ({skew:.2f})\")\n",
        "\n",
        "        # Correlation insights\n",
        "        numeric_data = self.data.select_dtypes(include=[np.number])\n",
        "        if len(numeric_data.columns) > 1:\n",
        "            corr_matrix = numeric_data.corr()\n",
        "            max_corr = corr_matrix.abs().unstack().sort_values(ascending=False)\n",
        "            max_corr = max_corr[max_corr < 1.0].iloc[0]\n",
        "            if max_corr > 0.7:\n",
        "                insights.append(f\"Strong correlation detected (r={max_corr:.2f}) between variables\")\n",
        "\n",
        "        # Categorical insights\n",
        "        categorical_columns = self.data.select_dtypes(include=['object', 'category']).columns\n",
        "        for column in categorical_columns:\n",
        "            unique_ratio = self.data[column].nunique() / len(self.data)\n",
        "            if unique_ratio > 0.8:\n",
        "                insights.append(f\"{column} has high cardinality ({self.data[column].nunique()} unique values)\")\n",
        "\n",
        "        return insights\n",
        "\n",
        "    def create_visualizations(self) -> Dict[str, Any]:\n",
        "        \"\"\"Create key visualizations\"\"\"\n",
        "        print(\"Creating visualizations...\")\n",
        "\n",
        "        # Set style\n",
        "        plt.style.use('seaborn-v0_8')\n",
        "        sns.set_palette(\"husl\")\n",
        "\n",
        "        visualizations = {}\n",
        "\n",
        "        # 1. Correlation heatmap\n",
        "        numeric_data = self.data.select_dtypes(include=[np.number])\n",
        "        if len(numeric_data.columns) > 1:\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm', center=0)\n",
        "            plt.title('Correlation Matrix')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('eda_output/correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            visualizations['correlation_heatmap'] = 'correlation_heatmap.png'\n",
        "\n",
        "        # 2. Distribution plots\n",
        "        numeric_columns = numeric_data.columns[:4]  # Limit to first 4 columns\n",
        "        if len(numeric_columns) > 0:\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "            axes = axes.flatten()\n",
        "\n",
        "            for i, column in enumerate(numeric_columns):\n",
        "                if i < 4:\n",
        "                    axes[i].hist(self.data[column], bins=30, alpha=0.7, color='skyblue')\n",
        "                    axes[i].set_title(f'Distribution of {column}')\n",
        "                    axes[i].set_ylabel('Frequency')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('eda_output/distributions.png', dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            visualizations['distributions'] = 'distributions.png'\n",
        "\n",
        "        # 3. Categorical variable analysis\n",
        "        categorical_columns = self.data.select_dtypes(include=['object', 'category']).columns\n",
        "        if len(categorical_columns) > 0:\n",
        "            fig, axes = plt.subplots(1, min(2, len(categorical_columns)), figsize=(12, 5))\n",
        "            if len(categorical_columns) == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            for i, column in enumerate(categorical_columns[:2]):\n",
        "                value_counts = self.data[column].value_counts()\n",
        "                axes[i].bar(value_counts.index, value_counts.values, alpha=0.7)\n",
        "                axes[i].set_title(f'Distribution of {column}')\n",
        "                axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('eda_output/categorical_analysis.png', dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            visualizations['categorical_analysis'] = 'categorical_analysis.png'\n",
        "\n",
        "        return visualizations\n",
        "\n",
        "    def generate_report(self) -> str:\n",
        "        \"\"\"Generate comprehensive EDA report\"\"\"\n",
        "        print(\"Generating comprehensive report...\")\n",
        "\n",
        "        report = f\"\"\"\n",
        "# Exploratory Data Analysis Report\n",
        "\n",
        "## Executive Summary\n",
        "This report presents a comprehensive analysis of the dataset containing {self.data.shape[0]} observations and {self.data.shape[1]} variables.\n",
        "\n",
        "## Dataset Overview\n",
        "- **Shape**: {self.data.shape[0]} rows × {self.data.shape[1]} columns\n",
        "- **Memory Usage**: {self.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n",
        "- **Missing Values**: {self.data.isnull().sum().sum()} total missing values\n",
        "\n",
        "## Data Quality Assessment\n",
        "\n",
        "### Missing Values\n",
        "{self._format_missing_values_summary()}\n",
        "\n",
        "### Data Types\n",
        "{self._format_data_types_summary()}\n",
        "\n",
        "### Outliers\n",
        "{self._format_outliers_summary()}\n",
        "\n",
        "## Statistical Summary\n",
        "\n",
        "### Numeric Variables\n",
        "{self._format_numeric_summary()}\n",
        "\n",
        "### Categorical Variables\n",
        "{self._format_categorical_summary()}\n",
        "\n",
        "## Key Insights\n",
        "{self._format_insights()}\n",
        "\n",
        "## Correlations\n",
        "{self._format_correlation_analysis()}\n",
        "\n",
        "## Recommendations\n",
        "{self._generate_recommendations()}\n",
        "\n",
        "## Methodology\n",
        "This analysis was conducted using a multi-agent system with the following components:\n",
        "- Data Preparation Agent: Handled data cleaning and preprocessing\n",
        "- EDA Agent: Performed statistical analysis and visualizations\n",
        "- Report Generator: Created this comprehensive report\n",
        "- Critic Agent: Provided quality assurance and feedback\n",
        "- Executor Agent: Validated code and results\n",
        "- Admin Agent: Coordinated the overall workflow\n",
        "\n",
        "## Conclusion\n",
        "{self._generate_conclusion()}\n",
        "\"\"\"\n",
        "\n",
        "        # Save report\n",
        "        os.makedirs('eda_output', exist_ok=True)\n",
        "        with open('eda_output/eda_report.md', 'w') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        return report\n",
        "\n",
        "    def _format_missing_values_summary(self) -> str:\n",
        "        \"\"\"Format missing values summary\"\"\"\n",
        "        missing_summary = self.data.isnull().sum()\n",
        "        if missing_summary.sum() == 0:\n",
        "            return \"No missing values detected in the dataset.\"\n",
        "\n",
        "        missing_info = []\n",
        "        for column, count in missing_summary.items():\n",
        "            if count > 0:\n",
        "                percentage = (count / len(self.data)) * 100\n",
        "                missing_info.append(f\"- {column}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "        return \"\\n\".join(missing_info)\n",
        "\n",
        "    def _format_data_types_summary(self) -> str:\n",
        "        \"\"\"Format data types summary\"\"\"\n",
        "        type_summary = self.data.dtypes.value_counts()\n",
        "        return \"\\n\".join([f\"- {dtype}: {count} columns\" for dtype, count in type_summary.items()])\n",
        "\n",
        "    def _format_outliers_summary(self) -> str:\n",
        "        \"\"\"Format outliers summary\"\"\"\n",
        "        if 'data_preparation' not in self.results:\n",
        "            return \"Outlier analysis not available.\"\n",
        "\n",
        "        outlier_info = self.results['data_preparation']['outliers']\n",
        "        outlier_summary = []\n",
        "\n",
        "        for column, info in outlier_info.items():\n",
        "            if info['count'] > 0:\n",
        "                outlier_summary.append(f\"- {column}: {info['count']} outliers ({info['percentage']:.1f}%)\")\n",
        "\n",
        "        return \"\\n\".join(outlier_summary) if outlier_summary else \"No significant outliers detected.\"\n",
        "\n",
        "    def _format_numeric_summary(self) -> str:\n",
        "        \"\"\"Format numeric variables summary\"\"\"\n",
        "        numeric_data = self.data.select_dtypes(include=[np.number])\n",
        "        if numeric_data.empty:\n",
        "            return \"No numeric variables found in the dataset.\"\n",
        "\n",
        "        summary = []\n",
        "        for column in numeric_data.columns:\n",
        "            stats = numeric_data[column].describe()\n",
        "            summary.append(f\"**{column}**:\")\n",
        "            summary.append(f\"  - Mean: {stats['mean']:.2f}\")\n",
        "            summary.append(f\"  - Median: {stats['50%']:.2f}\")\n",
        "            summary.append(f\"  - Std Dev: {stats['std']:.2f}\")\n",
        "            summary.append(f\"  - Range: [{stats['min']:.2f}, {stats['max']:.2f}]\")\n",
        "            summary.append(\"\")\n",
        "\n",
        "        return \"\\n\".join(summary)\n",
        "\n",
        "    def _format_categorical_summary(self) -> str:\n",
        "        \"\"\"Format categorical variables summary\"\"\"\n",
        "        categorical_data = self.data.select_dtypes(include=['object', 'category'])\n",
        "        if categorical_data.empty:\n",
        "            return \"No categorical variables found in the dataset.\"\n",
        "\n",
        "        summary = []\n",
        "        for column in categorical_data.columns:\n",
        "            unique_count = self.data[column].nunique()\n",
        "            most_frequent = self.data[column].mode().iloc[0] if not self.data[column].mode().empty else \"N/A\"\n",
        "            summary.append(f\"**{column}**:\")\n",
        "            summary.append(f\"  - Unique values: {unique_count}\")\n",
        "            summary.append(f\"  - Most frequent: {most_frequent}\")\n",
        "            summary.append(\"\")\n",
        "\n",
        "        return \"\\n\".join(summary)\n",
        "\n",
        "    def _format_insights(self) -> str:\n",
        "        \"\"\"Format key insights\"\"\"\n",
        "        if 'eda_analysis' not in self.results:\n",
        "            return \"Insights not available.\"\n",
        "\n",
        "        insights = self.results['eda_analysis']['insights']\n",
        "        return \"\\n\".join([f\"- {insight}\" for insight in insights])\n",
        "\n",
        "    def _format_correlation_analysis(self) -> str:\n",
        "        \"\"\"Format correlation analysis\"\"\"\n",
        "        if 'eda_analysis' not in self.results:\n",
        "            return \"Correlation analysis not available.\"\n",
        "\n",
        "        strong_correlations = self.results['eda_analysis']['correlations']['strong_correlations']\n",
        "        if not strong_correlations:\n",
        "            return \"No strong correlations (>0.5) detected between variables.\"\n",
        "\n",
        "        corr_summary = []\n",
        "        for corr in strong_correlations:\n",
        "            corr_summary.append(f\"- {corr['var1']} ↔ {corr['var2']}: {corr['correlation']:.3f}\")\n",
        "\n",
        "        return \"\\n\".join(corr_summary)\n",
        "\n",
        "    def _generate_recommendations(self) -> str:\n",
        "        \"\"\"Generate recommendations based on analysis\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        # Data quality recommendations\n",
        "        missing_data = self.data.isnull().sum().sum()\n",
        "        if missing_data > 0:\n",
        "            recommendations.append(\"- Address missing values through appropriate imputation or collection strategies\")\n",
        "\n",
        "        # Distribution recommendations\n",
        "        numeric_columns = self.data.select_dtypes(include=[np.number]).columns\n",
        "        for column in numeric_columns:\n",
        "            skew = self.data[column].skew()\n",
        "            if abs(skew) > 1:\n",
        "                recommendations.append(f\"- Consider transformation for {column} due to high skewness\")\n",
        "\n",
        "        # Correlation recommendations\n",
        "        if 'eda_analysis' in self.results:\n",
        "            strong_correlations = self.results['eda_analysis']['correlations']['strong_correlations']\n",
        "            if strong_correlations:\n",
        "                recommendations.append(\"- Investigate multicollinearity issues in predictive modeling\")\n",
        "\n",
        "        # General recommendations\n",
        "        recommendations.extend([\n",
        "            \"- Validate findings with domain experts\",\n",
        "            \"- Consider feature engineering opportunities\",\n",
        "            \"- Plan for appropriate statistical modeling approaches\"\n",
        "        ])\n",
        "\n",
        "        return \"\\n\".join(recommendations)\n",
        "\n",
        "    def _generate_conclusion(self) -> str:\n",
        "        \"\"\"Generate conclusion\"\"\"\n",
        "        return f\"\"\"\n",
        "The dataset contains {self.data.shape[0]} observations across {self.data.shape[1]} variables with varying data types and quality characteristics. The analysis reveals important patterns, relationships, and data quality issues that should inform subsequent modeling and analysis decisions.\n",
        "\n",
        "Key findings include data distribution patterns, correlation structures, and quality assessments that provide a solid foundation for further analytical work. The multi-agent approach ensured comprehensive coverage of all critical EDA components while maintaining quality and consistency throughout the analysis process.\n",
        "\"\"\"\n",
        "\n",
        "    def run_complete_analysis(self, data_path: Optional[str] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Run the complete EDA analysis workflow\"\"\"\n",
        "        print(\"Starting Complete EDA Analysis...\")\n",
        "\n",
        "        # Load data\n",
        "        if data_path:\n",
        "            self.load_data(data_path)\n",
        "        else:\n",
        "            self.create_sample_data()\n",
        "\n",
        "        # Run analysis phases\n",
        "        try:\n",
        "            # Phase 1: Data Preparation\n",
        "            prep_results = self.run_data_preparation()\n",
        "\n",
        "            # Phase 2: EDA Analysis\n",
        "            eda_results = self.run_eda_analysis()\n",
        "\n",
        "            # Phase 3: Create Visualizations\n",
        "            visualizations = self.create_visualizations()\n",
        "\n",
        "            # Phase 4: Generate Report\n",
        "            report = self.generate_report()\n",
        "\n",
        "            # Compile final results\n",
        "            final_results = {\n",
        "                \"data_preparation\": prep_results,\n",
        "                \"eda_analysis\": eda_results,\n",
        "                \"visualizations\": visualizations,\n",
        "                \"report\": report,\n",
        "                \"data_shape\": self.data.shape,\n",
        "                \"analysis_summary\": {\n",
        "                    \"total_variables\": self.data.shape[1],\n",
        "                    \"total_observations\": self.data.shape[0],\n",
        "                    \"missing_values\": self.data.isnull().sum().sum(),\n",
        "                    \"numeric_variables\": len(self.data.select_dtypes(include=[np.number]).columns),\n",
        "                    \"categorical_variables\": len(self.data.select_dtypes(include=['object', 'category']).columns)\n",
        "                }\n",
        "            }\n",
        "\n",
        "            print(\"Complete EDA Analysis finished successfully!\")\n",
        "            print(f\"Results saved to: eda_output/\")\n",
        "            print(f\"Report available at: eda_output/eda_report.md\")\n",
        "\n",
        "            return final_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during analysis: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "# Usage Example and Demo\n",
        "def main():\n",
        "    \"\"\"Main function to demonstrate the EDA system\"\"\"\n",
        "\n",
        "    # Initialize the system (replace with your actual Gemini API key)\n",
        "    GEMINI_API_KEY = \"your_gemini_api_key_here\"  # Replace with actual key\n",
        "\n",
        "    try:\n",
        "        # Create EDA system\n",
        "        eda_system = EDAMultiAgentSystem(GEMINI_API_KEY)\n",
        "\n",
        "        # Run complete analysis with sample data\n",
        "        results = eda_system.run_complete_analysis()\n",
        "\n",
        "        # Print summary\n",
        "        if \"error\" not in results:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"ANALYSIS SUMMARY\")\n",
        "            print(\"=\"*60)\n",
        "            print(f\"Dataset Shape: {results['data_shape']}\")\n",
        "            print(f\"Total Variables: {results['analysis_summary']['total_variables']}\")\n",
        "            print(f\"Total Observations: {results['analysis_summary']['total_observations']}\")\n",
        "            print(f\"Missing Values: {results['analysis_summary']['missing_values']}\")\n",
        "            print(f\"Numeric Variables: {results['analysis_summary']['numeric_variables']}\")\n",
        "            print(f\"Categorical Variables: {results['analysis_summary']['categorical_variables']}\")\n",
        "            print(\"\\nFiles generated:\")\n",
        "            print(\"- eda_output/eda_report.md\")\n",
        "            for viz_name, viz_file in results['visualizations'].items():\n",
        "                print(f\"- eda_output/{viz_file}\")\n",
        "\n",
        "            print(\"\\nKey Insights:\")\n",
        "            for insight in results['eda_analysis']['insights']:\n",
        "                print(f\"- {insight}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Analysis failed: {results['error']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"System initialization failed: {e}\")\n",
        "        print(\"Please ensure you have a valid Gemini API key and all required packages installed.\")\n",
        "\n",
        "# Additional utility functions for advanced analysis\n",
        "class AdvancedEDAFeatures:\n",
        "    \"\"\"Extended features for advanced EDA analysis\"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame):\n",
        "        self.data = data\n",
        "\n",
        "    def detect_data_drift(self, reference_data: pd.DataFrame) -> Dict[str, Any]:\n",
        "        \"\"\"Detect data drift between current and reference datasets\"\"\"\n",
        "        drift_results = {}\n",
        "\n",
        "        for column in self.data.columns:\n",
        "            if column in reference_data.columns:\n",
        "                if self.data[column].dtype in ['int64', 'float64']:\n",
        "                    # Statistical tests for numeric data\n",
        "                    from scipy import stats\n",
        "                    statistic, p_value = stats.ks_2samp(self.data[column].dropna(),\n",
        "                                                      reference_data[column].dropna())\n",
        "                    drift_results[column] = {\n",
        "                        'test': 'Kolmogorov-Smirnov',\n",
        "                        'statistic': statistic,\n",
        "                        'p_value': p_value,\n",
        "                        'drift_detected': p_value < 0.05\n",
        "                    }\n",
        "                else:\n",
        "                    # Chi-square test for categorical data\n",
        "                    current_counts = self.data[column].value_counts()\n",
        "                    reference_counts = reference_data[column].value_counts()\n",
        "\n",
        "                    # Align categories\n",
        "                    all_categories = set(current_counts.index) | set(reference_counts.index)\n",
        "                    current_aligned = [current_counts.get(cat, 0) for cat in all_categories]\n",
        "                    reference_aligned = [reference_counts.get(cat, 0) for cat in all_categories]\n",
        "\n",
        "                    if sum(current_aligned) > 0 and sum(reference_aligned) > 0:\n",
        "                        chi2, p_value = stats.chisquare(current_aligned, reference_aligned)\n",
        "                        drift_results[column] = {\n",
        "                            'test': 'Chi-square',\n",
        "                            'statistic': chi2,\n",
        "                            'p_value': p_value,\n",
        "                            'drift_detected': p_value < 0.05\n",
        "                        }\n",
        "\n",
        "        return drift_results\n",
        "\n",
        "    def feature_importance_analysis(self, target_column: str) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze feature importance using various methods\"\"\"\n",
        "        if target_column not in self.data.columns:\n",
        "            return {\"error\": f\"Target column '{target_column}' not found\"}\n",
        "\n",
        "        importance_results = {}\n",
        "\n",
        "        # Correlation-based importance for numeric target\n",
        "        if self.data[target_column].dtype in ['int64', 'float64']:\n",
        "            numeric_features = self.data.select_dtypes(include=[np.number]).columns\n",
        "            numeric_features = numeric_features.drop(target_column)\n",
        "\n",
        "            correlations = {}\n",
        "            for feature in numeric_features:\n",
        "                corr = self.data[feature].corr(self.data[target_column])\n",
        "                correlations[feature] = abs(corr)\n",
        "\n",
        "            importance_results['correlation_importance'] = dict(\n",
        "                sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
        "            )\n",
        "\n",
        "        # Mutual information for all features\n",
        "        from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "        # Prepare features\n",
        "        feature_data = self.data.drop(columns=[target_column])\n",
        "\n",
        "        # Encode categorical variables\n",
        "        encoded_features = feature_data.copy()\n",
        "        label_encoders = {}\n",
        "\n",
        "        for column in encoded_features.select_dtypes(include=['object', 'category']).columns:\n",
        "            le = LabelEncoder()\n",
        "            encoded_features[column] = le.fit_transform(encoded_features[column].astype(str))\n",
        "            label_encoders[column] = le\n",
        "\n",
        "        # Calculate mutual information\n",
        "        if self.data[target_column].dtype in ['int64', 'float64']:\n",
        "            mi_scores = mutual_info_regression(encoded_features, self.data[target_column])\n",
        "        else:\n",
        "            le_target = LabelEncoder()\n",
        "            encoded_target = le_target.fit_transform(self.data[target_column].astype(str))\n",
        "            mi_scores = mutual_info_classif(encoded_features, encoded_target)\n",
        "\n",
        "        mi_importance = dict(zip(encoded_features.columns, mi_scores))\n",
        "        importance_results['mutual_information'] = dict(\n",
        "                sorted(mi_importance.items(), key=lambda x: x[1], reverse=True)\n",
        "            )\n",
        "\n",
        "        return importance_results\n",
        "\n",
        "    def automated_insights_generator(self) -> List[str]:\n",
        "        \"\"\"Generate automated insights using statistical analysis\"\"\"\n",
        "        insights = []\n",
        "\n",
        "        # Dataset size insights\n",
        "        if self.data.shape[0] < 100:\n",
        "            insights.append(\"Small dataset size may limit statistical power of analyses\")\n",
        "        elif self.data.shape[0] > 100000:\n",
        "            insights.append(\"Large dataset provides good statistical power but may require sampling for visualization\")\n",
        "\n",
        "        # Missing data patterns\n",
        "        missing_pattern = self.data.isnull().sum()\n",
        "        if missing_pattern.sum() > 0:\n",
        "            worst_missing = missing_pattern.idxmax()\n",
        "            worst_pct = (missing_pattern.max() / len(self.data)) * 100\n",
        "            insights.append(f\"'{worst_missing}' has the highest missing data rate at {worst_pct:.1f}%\")\n",
        "\n",
        "        # Cardinality insights\n",
        "        for column in self.data.select_dtypes(include=['object', 'category']).columns:\n",
        "            cardinality = self.data[column].nunique()\n",
        "            if cardinality == len(self.data):\n",
        "                insights.append(f\"'{column}' appears to be a unique identifier\")\n",
        "            elif cardinality > 0.5 * len(self.data):\n",
        "                insights.append(f\"'{column}' has high cardinality ({cardinality} unique values)\")\n",
        "\n",
        "        # Numeric distribution insights\n",
        "        for column in self.data.select_dtypes(include=[np.number]).columns:\n",
        "            skewness = self.data[column].skew()\n",
        "            if abs(skewness) > 2:\n",
        "                insights.append(f\"'{column}' is highly skewed (skewness: {skewness:.2f})\")\n",
        "\n",
        "            # Check for potential log-normal distribution\n",
        "            if self.data[column].min() > 0 and skewness > 1:\n",
        "                log_skewness = np.log(self.data[column]).skew()\n",
        "                if abs(log_skewness) < abs(skewness):\n",
        "                    insights.append(f\"'{column}' may benefit from log transformation\")\n",
        "\n",
        "        # Correlation insights\n",
        "        numeric_data = self.data.select_dtypes(include=[np.number])\n",
        "        if len(numeric_data.columns) > 1:\n",
        "            corr_matrix = numeric_data.corr()\n",
        "\n",
        "            # Find perfect correlations (excluding diagonal)\n",
        "            perfect_corr = []\n",
        "            for i in range(len(corr_matrix.columns)):\n",
        "                for j in range(i+1, len(corr_matrix.columns)):\n",
        "                    if abs(corr_matrix.iloc[i, j]) > 0.95:\n",
        "                        perfect_corr.append((corr_matrix.columns[i], corr_matrix.columns[j]))\n",
        "\n",
        "            if perfect_corr:\n",
        "                insights.append(f\"Near-perfect correlations detected: {perfect_corr}\")\n",
        "\n",
        "        # Outlier insights\n",
        "        for column in self.data.select_dtypes(include=[np.number]).columns:\n",
        "            Q1 = self.data[column].quantile(0.25)\n",
        "            Q3 = self.data[column].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            outliers = ((self.data[column] < (Q1 - 1.5 * IQR)) |\n",
        "                         (self.data[column] > (Q3 + 1.5 * IQR))).sum()\n",
        "\n",
        "            if outliers > 0.05 * len(self.data):  # More than 5% outliers\n",
        "                insights.append(f\"'{column}' has {outliers} outliers ({outliers/len(self.data)*100:.1f}%)\")\n",
        "\n",
        "        return insights\n",
        "\n",
        "# Integration with existing system\n",
        "def enhanced_eda_system_example():\n",
        "    \"\"\"Example of enhanced EDA system with advanced features\"\"\"\n",
        "\n",
        "    # Create sample data with more complex patterns\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "\n",
        "    # Generate correlated features\n",
        "    feature1 = np.random.normal(0, 1, n_samples)\n",
        "    feature2 = 0.8 * feature1 + np.random.normal(0, 0.5, n_samples)  # Correlated\n",
        "    feature3 = np.random.exponential(2, n_samples)  # Skewed\n",
        "\n",
        "    enhanced_data = pd.DataFrame({\n",
        "        'feature1': feature1,\n",
        "        'feature2': feature2,\n",
        "        'feature3': feature3,\n",
        "        'target': 2 * feature1 + 1.5 * feature2 + np.random.normal(0, 0.5, n_samples),\n",
        "        'category': np.random.choice(['A', 'B', 'C'], n_samples, p=[0.5, 0.3, 0.2]),\n",
        "        'high_cardinality': [f'ID_{i}' for i in range(n_samples)],  # Unique IDs\n",
        "        'binary_feature': np.random.choice([0, 1], n_samples)\n",
        "    })\n",
        "\n",
        "    # Add missing values\n",
        "    enhanced_data.loc[np.random.choice(n_samples, 50, replace=False), 'feature3'] = np.nan\n",
        "\n",
        "    # Create advanced EDA features\n",
        "    advanced_eda = AdvancedEDAFeatures(enhanced_data)\n",
        "\n",
        "    # Generate automated insights\n",
        "    insights = advanced_eda.automated_insights_generator()\n",
        "\n",
        "    print(\"Enhanced EDA - Automated Insights:\")\n",
        "    for insight in insights:\n",
        "        print(f\"- {insight}\")\n",
        "\n",
        "    # Feature importance analysis\n",
        "    importance_results = advanced_eda.feature_importance_analysis('target')\n",
        "\n",
        "    print(\"\\nFeature Importance Analysis:\")\n",
        "    if 'correlation_importance' in importance_results:\n",
        "        print(\"Top correlated features:\")\n",
        "        for feature, importance in list(importance_results['correlation_importance'].items())[:5]:\n",
        "            print(f\"  {feature}: {importance:.3f}\")\n",
        "\n",
        "    if 'mutual_information' in importance_results:\n",
        "        print(\"Top mutual information features:\")\n",
        "        for feature, importance in list(importance_results['mutual_information'].items())[:5]:\n",
        "            print(f\"  {feature}: {importance:.3f}\")\n",
        "\n",
        "# Configuration and Setup Instructions\n",
        "def setup_instructions():\n",
        "    \"\"\"Print setup instructions for the EDA system\"\"\"\n",
        "\n",
        "    instructions = \"\"\"\n",
        "# Multi-Agent EDA System Setup Instructions\n",
        "\n",
        "## Required Dependencies\n",
        "\n",
        "Install the following packages:"
      ]
    }
  ]
}